// Lexer - Lexical analyzer for Aster language
// Part of Aster Stage 1 (Core-0 implementation)
//
// This module implements the lexer that processes UTF-8 input and produces
// a stream of tokens with full span tracking.
//
// Language Subset: Core-0
// - Uses only: structs, enums, String, functions, while loops
// - No traits, no HashMap, no methods, no tuples
// - All functions are standalone
// - Self-contained for Stage 0 compilation (includes all dependencies inline)
// - Note: Vec<Token> return type relies on C# compiler support during bootstrap

// ============================================================================
// EMBEDDED DEPENDENCIES (for self-contained compilation)
// ============================================================================

/// Source location span within a file
struct Span {
    file: String,
    line: i32,
    column: i32,
    start: i32,
    length: i32
}

/// Create a new Span
fn new_span(file: String, line: i32, column: i32, start: i32, length: i32) -> Span {
    Span {
        file: file,
        line: line,
        column: column,
        start: start,
        length: length
    }
}

/// Token type enumeration
enum TokenKind {
    // Literals
    Identifier,
    IntegerLiteral,
    FloatLiteral,
    StringLiteral,
    CharLiteral,
    // Keywords
    Fn,
    Let,
    Mut,
    Type,
    Trait,
    Impl,
    Match,
    If,
    Else,
    For,
    While,
    Return,
    Break,
    Continue,
    Loop,
    Async,
    Await,
    Actor,
    Module,
    Pub,
    Extern,
    Unsafe,
    Using,
    Managed,
    Throws,
    Struct,
    Enum,
    True,
    False,
    // Operators
    Plus,
    Minus,
    Star,
    Slash,
    Percent,
    Ampersand,
    Pipe,
    Caret,
    Tilde,
    Bang,
    Less,
    Greater,
    Equals,
    Dot,
    DotDot,
    // Compound operators
    AmpersandAmpersand,
    PipePipe,
    EqualsEquals,
    BangEquals,
    LessEquals,
    GreaterEquals,
    Arrow,
    FatArrow,
    PlusEquals,
    MinusEquals,
    StarEquals,
    SlashEquals,
    ColonColon,
    // Punctuation
    LeftParen,
    RightParen,
    LeftBrace,
    RightBrace,
    LeftBracket,
    RightBracket,
    Comma,
    Colon,
    Semicolon,
    At,
    Hash,
    Underscore,
    // Special
    Eof,
    Error
}

/// Lexical token
struct Token {
    kind: TokenKind,
    span: Span,
    value: String
}

/// Create a new token
fn new_token(kind: TokenKind, span: Span, value: String) -> Token {
    Token {
        kind: kind,
        span: span,
        value: value
    }
}

/// Helper to compare two TokenKinds for equality
fn token_kind_equals(a: TokenKind, b: TokenKind) -> bool {
    // Simplified for Core-0
    false
}

/// Create an EOF token
fn make_eof_token(file: String, position: i32) -> Token {
    let empty = "";
    let span = new_span(empty, 0, 0, position, 0);
    new_token(TokenKind::Eof, span, empty)
}

/// Create an error token
fn make_error_token(span: Span, message: String) -> Token {
    new_token(TokenKind::Error, span, message)
}

/// String interner for deduplication
/// Simplified for Core-0 - counter-based, no actual pooling
struct StringInterner {
    count: i32
}

/// Result of string interning
struct InternResult {
    interner: StringInterner,
    value: String
}

/// Create a new empty string interner
fn new_interner() -> StringInterner {
    StringInterner {
        count: 0
    }
}

/// Intern a string
/// Simplified for Core-0 - just passes through the string and increments counter
fn intern(mut interner: StringInterner, value: String) -> InternResult {
    interner.count = interner.count + 1;
    InternResult {
        interner: interner,
        value: value
    }
}

/// Intern a substring
fn intern_substring(interner: StringInterner, source: String, start: i32, end: i32) -> InternResult {
    let substring = string_substring(source, start, end);
    intern(interner, substring)
}

// ============================================================================
// LEXER IMPLEMENTATION
// ============================================================================

/// Lexer state for tokenizing Aster source code.
struct Lexer {
    source: String,
    file_name: String,
    interner: StringInterner,
    position: i32,
    line: i32,
    column: i32
}

/// Result of lexing operation that returns both lexer and token
/// (Core-0 doesn't support tuple returns)
struct LexerTokenResult {
    lexer: Lexer,
    token: Token
}

/// Create a new lexer for the given source code.
fn new_lexer(source: String, file_name: String) -> Lexer {
    Lexer {
        source: source,
        file_name: file_name,
        interner: new_interner(),
        position: 0,
        line: 1,
        column: 1
    }
}

/// Tokenize the entire source code and return a list of tokens.
/// Simplified stub for Core-0 bootstrap - full implementation requires Vec which isn't available
/// The real tokenization happens through next_token() calls in the actual compiler
fn tokenize(mut lexer: Lexer) -> Token {
    // Stub: Just return first token for Core-0 compilation
    // In actual usage, caller would repeatedly call next_token() instead
    let result = next_token(lexer);
    result.token
}

/// Get the next token from the source.
/// Returns LexerTokenResult with updated lexer and token
fn next_token(mut lexer: Lexer) -> LexerTokenResult {
    lexer = skip_whitespace_and_comments(lexer);
    
    // Check for end of file
    if lexer.position >= string_length(lexer.source) {
        let token = make_eof_token(lexer.file_name, lexer.position);
        return LexerTokenResult { lexer: lexer, token: token };
    }
    
    let start = lexer.position;
    let start_line = lexer.line;
    let start_column = lexer.column;
    let ch = current_char_from_lexer(lexer);
    
    // Identifiers and keywords
    if is_ident_start(ch) {
        return lex_identifier_or_keyword(lexer);
    }
    
    // Number literals
    if is_digit(ch) {
        return lex_number(lexer);
    }
    
    // String literals
    if ch == '"' {
        return lex_string(lexer);
    }
    
    // Char literals
    if ch == '\'' {
        return lex_char(lexer);
    }
    
    // Operators and punctuation
    lex_operator_or_punctuation(lexer)
}

/// Lex an identifier or keyword.
/// Returns LexerTokenResult with updated lexer and token
fn lex_identifier_or_keyword(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    
    // Consume identifier characters
    while lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        if !is_ident_continue(ch) {
            break;
        }
        lexer = advance(lexer);
    }
    
    // Extract and intern the text
    let result = intern_substring(lexer.interner, lexer.source, start, lexer.position);
    lexer.interner = result.interner;
    let text = result.value;
    
    // Check if it's a keyword
    let kind = keyword_lookup(text);
    
    let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
    let token = new_token(kind, span, text);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex a number literal (integer or float).
/// Returns LexerTokenResult with updated lexer and token
fn lex_number(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    let mut is_float = false;
    
    // Handle hex/binary prefixes
    if current_char_from_lexer(lexer) == '0' && lexer.position + 1 < string_length(lexer.source) {
        let next = char_at(lexer.source, lexer.position + 1);
        
        // Hexadecimal: 0x...
        if next == 'x' || next == 'X' {
            lexer = advance(lexer); // skip '0'
            lexer = advance(lexer); // skip 'x'
            while lexer.position < string_length(lexer.source) {
                let ch = char_at(lexer.source, lexer.position);
                if !is_hex_digit(ch) {
                    break;
                }
                lexer = advance(lexer);
            }
            let text = string_substring(lexer.source, start, lexer.position);
            let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
            let token = new_token(TokenKind::IntegerLiteral, span, text);
            return LexerTokenResult { lexer: lexer, token: token };
        }
        
        // Binary: 0b...
        if next == 'b' || next == 'B' {
            lexer = advance(lexer); // skip '0'
            lexer = advance(lexer); // skip 'b'
            while lexer.position < string_length(lexer.source) {
                let ch = char_at(lexer.source, lexer.position);
                if ch != '0' && ch != '1' && ch != '_' {
                    break;
                }
                lexer = advance(lexer);
            }
            let text = string_substring(lexer.source, start, lexer.position);
            let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
            let token = new_token(TokenKind::IntegerLiteral, span, text);
            return LexerTokenResult { lexer: lexer, token: token };
        }
    }
    
    // Decimal digits
    while lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        if !is_digit(ch) && ch != '_' {
            break;
        }
        lexer = advance(lexer);
    }
    
    // Decimal point (for floats)
    if lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        if ch == '.' && lexer.position + 1 < string_length(lexer.source) {
            let next = char_at(lexer.source, lexer.position + 1);
            if is_digit(next) {
                is_float = true;
                lexer = advance(lexer); // skip '.'
                while lexer.position < string_length(lexer.source) {
                    let ch = char_at(lexer.source, lexer.position);
                    if !is_digit(ch) && ch != '_' {
                        break;
                    }
                    lexer = advance(lexer);
                }
            }
        }
    }
    
    // Exponent (e or E)
    if lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        if ch == 'e' || ch == 'E' {
            is_float = true;
            lexer = advance(lexer);
            if lexer.position < string_length(lexer.source) {
                let ch = char_at(lexer.source, lexer.position);
                if ch == '+' || ch == '-' {
                    lexer = advance(lexer);
                }
            }
            while lexer.position < string_length(lexer.source) {
                let ch = char_at(lexer.source, lexer.position);
                if !is_digit(ch) {
                    break;
                }
                lexer = advance(lexer);
            }
        }
    }
    
    let text = string_substring(lexer.source, start, lexer.position);
    let kind = if is_float { TokenKind::FloatLiteral } else { TokenKind::IntegerLiteral };
    let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
    let token = new_token(kind, span, text);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex a string literal with escape sequences.
/// Returns LexerTokenResult with updated lexer and token
fn lex_string(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    
    lexer = advance(lexer); // skip opening '"'
    
    let mut result = String::new();
    
    while lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        if ch == '"' {
            break;
        }
        
        if ch == '\\' {
            // Escape sequence
            lexer = advance(lexer);
            if lexer.position < string_length(lexer.source) {
                let esc = char_at(lexer.source, lexer.position);
                let escaped_char = match esc {
                    'n' => '\n',
                    'r' => '\r',
                    't' => '\t',
                    '\\' => '\\',
                    '"' => '"',
                    '0' => '\0',
                    _ => esc
                };
                result.push(escaped_char);
                lexer = advance(lexer);
            }
        } else {
            result.push(ch);
            lexer = advance(lexer);
        }
    }
    
    // Check for unterminated string
    if lexer.position >= string_length(lexer.source) {
        // Error: unterminated string
        let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
        let token = make_error_token(span, String::from("Unterminated string literal"));
        return LexerTokenResult { lexer: lexer, token: token };
    }
    
    lexer = advance(lexer); // skip closing '"'
    let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
    let token = new_token(TokenKind::StringLiteral, span, result);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex a character literal.
/// Returns LexerTokenResult with updated lexer and token
fn lex_char(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    
    lexer = advance(lexer); // skip opening '\''
    
    let mut value = String::new();
    
    if lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        
        if ch == '\\' {
            // Escape sequence
            lexer = advance(lexer);
            if lexer.position < string_length(lexer.source) {
                let esc = char_at(lexer.source, lexer.position);
                let escaped_char = match esc {
                    'n' => '\n',
                    'r' => '\r',
                    't' => '\t',
                    '\\' => '\\',
                    '\'' => '\'',
                    '0' => '\0',
                    _ => esc
                };
                value.push(escaped_char);
                lexer = advance(lexer);
            }
        } else {
            value.push(ch);
            lexer = advance(lexer);
        }
    }
    
    // Check for closing quote
    if lexer.position >= string_length(lexer.source) || char_at(lexer.source, lexer.position) != '\'' {
        let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
        let token = make_error_token(span, String::from("Unterminated character literal"));
        return LexerTokenResult { lexer: lexer, token: token };
    }
    
    lexer = advance(lexer); // skip closing '\''
    let span = new_span(lexer.file_name, start_line, start_col, start, lexer.position - start);
    let token = new_token(TokenKind::CharLiteral, span, value);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex an operator or punctuation token.
/// Returns LexerTokenResult with updated lexer and token
fn lex_operator_or_punctuation(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    let ch = current_char_from_lexer(lexer);
    
    lexer = advance(lexer);
    
    // Two-character operators
    if lexer.position < string_length(lexer.source) {
        let next = char_at(lexer.source, lexer.position);
        
        // Check all two-character operators
        if ch == '&' && next == '&' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::AmpersandAmpersand, span, String::from("&&"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '|' && next == '|' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::PipePipe, span, String::from("||"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '=' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::EqualsEquals, span, String::from("=="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '!' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::BangEquals, span, String::from("!="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '<' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::LessEquals, span, String::from("<="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '>' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::GreaterEquals, span, String::from(">="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '-' && next == '>' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::Arrow, span, String::from("->"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '=' && next == '>' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::FatArrow, span, String::from("=>"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '+' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::PlusEquals, span, String::from("+="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '-' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::MinusEquals, span, String::from("-="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '*' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::StarEquals, span, String::from("*="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '/' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::SlashEquals, span, String::from("/="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == ':' && next == ':' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::ColonColon, span, String::from("::"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '.' && next == '.' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name, start_line, start_col, start, 2);
            let token = new_token(TokenKind::DotDot, span, String::from(".."));
            return LexerTokenResult { lexer: lexer, token: token };
        }
    }
    
    // Single-character operators and punctuation
    let kind = match ch {
        '+' => TokenKind::Plus,
        '-' => TokenKind::Minus,
        '*' => TokenKind::Star,
        '/' => TokenKind::Slash,
        '%' => TokenKind::Percent,
        '&' => TokenKind::Ampersand,
        '|' => TokenKind::Pipe,
        '^' => TokenKind::Caret,
        '~' => TokenKind::Tilde,
        '!' => TokenKind::Bang,
        '<' => TokenKind::Less,
        '>' => TokenKind::Greater,
        '=' => TokenKind::Equals,
        '.' => TokenKind::Dot,
        '(' => TokenKind::LeftParen,
        ')' => TokenKind::RightParen,
        '{' => TokenKind::LeftBrace,
        '}' => TokenKind::RightBrace,
        '[' => TokenKind::LeftBracket,
        ']' => TokenKind::RightBracket,
        ',' => TokenKind::Comma,
        ':' => TokenKind::Colon,
        ';' => TokenKind::Semicolon,
        '@' => TokenKind::At,
        '#' => TokenKind::Hash,
        _ => TokenKind::Error
    };
    
    let mut char_str = String::new();
    char_str.push(ch);
    let span = new_span(lexer.file_name, start_line, start_col, start, 1);
    let token = new_token(kind, span, char_str);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Skip whitespace and comments.
/// Returns updated lexer
fn skip_whitespace_and_comments(mut lexer: Lexer) -> Lexer {
    while lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        
        // Whitespace
        if is_whitespace(ch) {
            lexer = advance(lexer);
            continue;
        }
        
        // Line comment: //
        if ch == '/' && lexer.position + 1 < string_length(lexer.source) {
            let next = char_at(lexer.source, lexer.position + 1);
            if next == '/' {
                // Skip until end of line
                while lexer.position < string_length(lexer.source) {
                    let c = char_at(lexer.source, lexer.position);
                    if c == '\n' {
                        break;
                    }
                    lexer = advance(lexer);
                }
                continue;
            }
            
            // Block comment: /* */
            if next == '*' {
                lexer = advance(lexer); // skip '/'
                lexer = advance(lexer); // skip '*'
                let mut depth = 1;
                
                while lexer.position < string_length(lexer.source) && depth > 0 {
                    let c = char_at(lexer.source, lexer.position);
                    
                    // Nested block comment start
                    if c == '/' && lexer.position + 1 < string_length(lexer.source) {
                        let n = char_at(lexer.source, lexer.position + 1);
                        if n == '*' {
                            depth = depth + 1;
                            lexer = advance(lexer);
                            lexer = advance(lexer);
                            continue;
                        }
                    }
                    
                    // Block comment end
                    if c == '*' && lexer.position + 1 < string_length(lexer.source) {
                        let n = char_at(lexer.source, lexer.position + 1);
                        if n == '/' {
                            depth = depth - 1;
                            lexer = advance(lexer);
                            lexer = advance(lexer);
                            continue;
                        }
                    }
                    
                    lexer = advance(lexer);
                }
                continue;
            }
        }
        
        // Not whitespace or comment
        break;
    }
    lexer
}

/// Advance position, tracking line and column.
/// Returns updated lexer
fn advance(mut lexer: Lexer) -> Lexer {
    if lexer.position < string_length(lexer.source) {
        let ch = char_at(lexer.source, lexer.position);
        if ch == '\n' {
            lexer.line = lexer.line + 1;
            lexer.column = 1;
        } else {
            lexer.column = lexer.column + 1;
        }
        lexer.position = lexer.position + 1;
    }
    lexer
}

/// Get the current character without advancing.
fn current_char_from_lexer(lexer: Lexer) -> char {
    if lexer.position < string_length(lexer.source) {
        char_at(lexer.source, lexer.position)
    } else {
        '\0'
    }
}

// ============================================================================
// HELPER FUNCTIONS (Core-0 doesn't have string methods)
// ============================================================================

/// Get length of a string as i32
fn string_length(s: String) -> i32 {
    // Simplified for Core-0 - relies on compiler to provide len()
    // The C# compiler will handle this during bootstrap
    let len = s.len();
    len
}

/// Get character at index in string
fn char_at(s: String, index: i32) -> char {
    let bytes = s.as_bytes();
    if index >= 0 && index < string_length(s) {
        // Note: The C# compiler will handle the i32 to array index conversion
        let byte = bytes[index];
        // Return byte directly - C# compiler handles u8 to char conversion
        byte
    } else {
        '\0'
    }
}

/// Extract substring from String
fn string_substring(s: String, start: i32, end: i32) -> String {
    let bytes = s.as_bytes();
    let mut result = String::new();
    
    let mut i = start;
    let bytes_len = string_length(s);
    while i < end && i < bytes_len {
        let byte = bytes[i];
        // Push byte directly - C# compiler handles u8 to char conversion
        result.push(byte);
        i = i + 1;
    }
    
    result
}

// ============================================================================
// CHARACTER CLASSIFICATION HELPERS
// ============================================================================

fn is_whitespace(ch: char) -> bool {
    ch == ' ' || ch == '\t' || ch == '\n' || ch == '\r'
}

fn is_digit(ch: char) -> bool {
    ch >= '0' && ch <= '9'
}

fn is_hex_digit(ch: char) -> bool {
    (ch >= '0' && ch <= '9') || 
    (ch >= 'a' && ch <= 'f') || 
    (ch >= 'A' && ch <= 'F') ||
    ch == '_'
}

fn is_ident_start(ch: char) -> bool {
    (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') || ch == '_'
}

fn is_ident_continue(ch: char) -> bool {
    is_ident_start(ch) || is_digit(ch)
}

/// Lookup a keyword by text. Returns Identifier if not a keyword.
fn keyword_lookup(text: String) -> TokenKind {
    // In Core-0, we can't use HashMap, so we do manual comparison
    // This is a simplified version - in reality we'd check all keywords
    
    if text == "fn" { return TokenKind::Fn; }
    if text == "let" { return TokenKind::Let; }
    if text == "mut" { return TokenKind::Mut; }
    if text == "type" { return TokenKind::Type; }
    if text == "trait" { return TokenKind::Trait; }
    if text == "impl" { return TokenKind::Impl; }
    if text == "match" { return TokenKind::Match; }
    if text == "if" { return TokenKind::If; }
    if text == "else" { return TokenKind::Else; }
    if text == "for" { return TokenKind::For; }
    if text == "while" { return TokenKind::While; }
    if text == "return" { return TokenKind::Return; }
    if text == "break" { return TokenKind::Break; }
    if text == "continue" { return TokenKind::Continue; }
    if text == "async" { return TokenKind::Async; }
    if text == "await" { return TokenKind::Await; }
    if text == "actor" { return TokenKind::Actor; }
    if text == "module" { return TokenKind::Module; }
    if text == "pub" { return TokenKind::Pub; }
    if text == "extern" { return TokenKind::Extern; }
    if text == "unsafe" { return TokenKind::Unsafe; }
    if text == "using" { return TokenKind::Using; }
    if text == "managed" { return TokenKind::Managed; }
    if text == "throws" { return TokenKind::Throws; }
    if text == "struct" { return TokenKind::Struct; }
    if text == "enum" { return TokenKind::Enum; }
    if text == "true" { return TokenKind::True; }
    if text == "false" { return TokenKind::False; }
    
    TokenKind::Identifier
}
