// Lexer - Lexical analyzer for Aster language
// Part of Aster Stage 1 (Core-0 implementation)
//
// This module implements the lexer that processes UTF-8 input and produces
// a stream of tokens with full span tracking.
//
// Language Subset: Core-0
// - Uses only: structs, enums, Vec, String, functions, while loops
// - No traits, no HashMap, no methods, no tuples
// - All functions are standalone

// Import dependencies (conceptual - actual imports would be handled differently)
// use crate::contracts::span::*;
// use crate::contracts::token::*;
// use crate::contracts::token_kind::*;
// use crate::frontend::string_interner::*;

/// Lexer state for tokenizing Aster source code.
struct Lexer {
    source: String,
    file_name: String,
    interner: StringInterner,
    position: i32,
    line: i32,
    column: i32
}

/// Result of lexing operation that returns both lexer and token
/// (Core-0 doesn't support tuple returns)
struct LexerTokenResult {
    lexer: Lexer,
    token: Token
}

/// Create a new lexer for the given source code.
fn new_lexer(source: String, file_name: String) -> Lexer {
    Lexer {
        source: source,
        file_name: file_name,
        interner: new_interner(),
        position: 0,
        line: 1,
        column: 1
    }
}

/// Tokenize the entire source code and return a list of tokens.
fn tokenize(mut lexer: Lexer) -> Vec<Token> {
    let mut tokens = Vec::new();
    
    loop {
        let result = next_token(lexer);
        lexer = result.lexer;
        let token = result.token;
        let is_eof = token_kind_equals(token.kind, TokenKind::Eof);
        tokens.push(token);
        if is_eof {
            break;
        }
    }
    
    tokens
}

/// Get the next token from the source.
/// Returns LexerTokenResult with updated lexer and token
fn next_token(mut lexer: Lexer) -> LexerTokenResult {
    lexer = skip_whitespace_and_comments(lexer);
    
    // Check for end of file
    if lexer.position >= string_length(lexer.source.clone()) {
        let token = make_eof_token(lexer.file_name.clone(), lexer.position);
        return LexerTokenResult { lexer: lexer, token: token };
    }
    
    let start = lexer.position;
    let start_line = lexer.line;
    let start_column = lexer.column;
    let ch = current_char_from_lexer(lexer.clone());
    
    // Identifiers and keywords
    if is_ident_start(ch) {
        return lex_identifier_or_keyword(lexer);
    }
    
    // Number literals
    if is_digit(ch) {
        return lex_number(lexer);
    }
    
    // String literals
    if ch == '"' {
        return lex_string(lexer);
    }
    
    // Char literals
    if ch == '\'' {
        return lex_char(lexer);
    }
    
    // Operators and punctuation
    lex_operator_or_punctuation(lexer)
}

/// Lex an identifier or keyword.
/// Returns LexerTokenResult with updated lexer and token
fn lex_identifier_or_keyword(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    
    // Consume identifier characters
    while lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        if !is_ident_continue(ch) {
            break;
        }
        lexer = advance(lexer);
    }
    
    // Extract and intern the text
    let result = intern_substring(lexer.interner, lexer.source.clone(), start, lexer.position);
    lexer.interner = result.interner;
    let text = result.value;
    
    // Check if it's a keyword
    let kind = keyword_lookup(text.clone());
    
    let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
    let token = new_token(kind, span, text);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex a number literal (integer or float).
/// Returns LexerTokenResult with updated lexer and token
fn lex_number(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    let mut is_float = false;
    
    // Handle hex/binary prefixes
    if current_char_from_lexer(lexer.clone()) == '0' && lexer.position + 1 < string_length(lexer.source.clone()) {
        let next = char_at(lexer.source.clone(), lexer.position + 1);
        
        // Hexadecimal: 0x...
        if next == 'x' || next == 'X' {
            lexer = advance(lexer); // skip '0'
            lexer = advance(lexer); // skip 'x'
            while lexer.position < string_length(lexer.source.clone()) {
                let ch = char_at(lexer.source.clone(), lexer.position);
                if !is_hex_digit(ch) {
                    break;
                }
                lexer = advance(lexer);
            }
            let text = string_substring(lexer.source.clone(), start, lexer.position);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
            let token = new_token(TokenKind::IntegerLiteral, span, text);
            return LexerTokenResult { lexer: lexer, token: token };
        }
        
        // Binary: 0b...
        if next == 'b' || next == 'B' {
            lexer = advance(lexer); // skip '0'
            lexer = advance(lexer); // skip 'b'
            while lexer.position < string_length(lexer.source.clone()) {
                let ch = char_at(lexer.source.clone(), lexer.position);
                if ch != '0' && ch != '1' && ch != '_' {
                    break;
                }
                lexer = advance(lexer);
            }
            let text = string_substring(lexer.source.clone(), start, lexer.position);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
            let token = new_token(TokenKind::IntegerLiteral, span, text);
            return LexerTokenResult { lexer: lexer, token: token };
        }
    }
    
    // Decimal digits
    while lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        if !is_digit(ch) && ch != '_' {
            break;
        }
        lexer = advance(lexer);
    }
    
    // Decimal point (for floats)
    if lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        if ch == '.' && lexer.position + 1 < string_length(lexer.source.clone()) {
            let next = char_at(lexer.source.clone(), lexer.position + 1);
            if is_digit(next) {
                is_float = true;
                lexer = advance(lexer); // skip '.'
                while lexer.position < string_length(lexer.source.clone()) {
                    let ch = char_at(lexer.source.clone(), lexer.position);
                    if !is_digit(ch) && ch != '_' {
                        break;
                    }
                    lexer = advance(lexer);
                }
            }
        }
    }
    
    // Exponent (e or E)
    if lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        if ch == 'e' || ch == 'E' {
            is_float = true;
            lexer = advance(lexer);
            if lexer.position < string_length(lexer.source.clone()) {
                let ch = char_at(lexer.source.clone(), lexer.position);
                if ch == '+' || ch == '-' {
                    lexer = advance(lexer);
                }
            }
            while lexer.position < string_length(lexer.source.clone()) {
                let ch = char_at(lexer.source.clone(), lexer.position);
                if !is_digit(ch) {
                    break;
                }
                lexer = advance(lexer);
            }
        }
    }
    
    let text = string_substring(lexer.source.clone(), start, lexer.position);
    let kind = if is_float { TokenKind::FloatLiteral } else { TokenKind::IntegerLiteral };
    let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
    let token = new_token(kind, span, text);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex a string literal with escape sequences.
/// Returns LexerTokenResult with updated lexer and token
fn lex_string(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    
    lexer = advance(lexer); // skip opening '"'
    
    let mut result = String::new();
    
    while lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        if ch == '"' {
            break;
        }
        
        if ch == '\\' {
            // Escape sequence
            lexer = advance(lexer);
            if lexer.position < string_length(lexer.source.clone()) {
                let esc = char_at(lexer.source.clone(), lexer.position);
                let escaped_char = match esc {
                    'n' => '\n',
                    'r' => '\r',
                    't' => '\t',
                    '\\' => '\\',
                    '"' => '"',
                    '0' => '\0',
                    _ => esc
                };
                result.push(escaped_char);
                lexer = advance(lexer);
            }
        } else {
            result.push(ch);
            lexer = advance(lexer);
        }
    }
    
    // Check for unterminated string
    if lexer.position >= string_length(lexer.source.clone()) {
        // Error: unterminated string
        let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
        let token = make_error_token(span, String::from("Unterminated string literal"));
        return LexerTokenResult { lexer: lexer, token: token };
    }
    
    lexer = advance(lexer); // skip closing '"'
    let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
    let token = new_token(TokenKind::StringLiteral, span, result);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex a character literal.
/// Returns LexerTokenResult with updated lexer and token
fn lex_char(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    
    lexer = advance(lexer); // skip opening '\''
    
    let mut value = String::new();
    
    if lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        
        if ch == '\\' {
            // Escape sequence
            lexer = advance(lexer);
            if lexer.position < string_length(lexer.source.clone()) {
                let esc = char_at(lexer.source.clone(), lexer.position);
                let escaped_char = match esc {
                    'n' => '\n',
                    'r' => '\r',
                    't' => '\t',
                    '\\' => '\\',
                    '\'' => '\'',
                    '0' => '\0',
                    _ => esc
                };
                value.push(escaped_char);
                lexer = advance(lexer);
            }
        } else {
            value.push(ch);
            lexer = advance(lexer);
        }
    }
    
    // Check for closing quote
    if lexer.position >= string_length(lexer.source.clone()) || char_at(lexer.source.clone(), lexer.position) != '\'' {
        let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
        let token = make_error_token(span, String::from("Unterminated character literal"));
        return LexerTokenResult { lexer: lexer, token: token };
    }
    
    lexer = advance(lexer); // skip closing '\''
    let span = new_span(lexer.file_name.clone(), start_line, start_col, start, lexer.position - start);
    let token = new_token(TokenKind::CharLiteral, span, value);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Lex an operator or punctuation token.
/// Returns LexerTokenResult with updated lexer and token
fn lex_operator_or_punctuation(mut lexer: Lexer) -> LexerTokenResult {
    let start = lexer.position;
    let start_line = lexer.line;
    let start_col = lexer.column;
    let ch = current_char_from_lexer(lexer.clone());
    
    lexer = advance(lexer);
    
    // Two-character operators
    if lexer.position < string_length(lexer.source.clone()) {
        let next = char_at(lexer.source.clone(), lexer.position);
        
        // Check all two-character operators
        if ch == '&' && next == '&' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::AmpersandAmpersand, span, String::from("&&"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '|' && next == '|' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::PipePipe, span, String::from("||"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '=' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::EqualsEquals, span, String::from("=="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '!' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::BangEquals, span, String::from("!="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '<' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::LessEquals, span, String::from("<="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '>' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::GreaterEquals, span, String::from(">="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '-' && next == '>' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::Arrow, span, String::from("->"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '=' && next == '>' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::FatArrow, span, String::from("=>"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '+' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::PlusEquals, span, String::from("+="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '-' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::MinusEquals, span, String::from("-="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '*' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::StarEquals, span, String::from("*="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '/' && next == '=' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::SlashEquals, span, String::from("/="));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == ':' && next == ':' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::ColonColon, span, String::from("::"));
            return LexerTokenResult { lexer: lexer, token: token };
        }
        if ch == '.' && next == '.' {
            lexer = advance(lexer);
            let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 2);
            let token = new_token(TokenKind::DotDot, span, String::from(".."));
            return LexerTokenResult { lexer: lexer, token: token };
        }
    }
    
    // Single-character operators and punctuation
    let kind = match ch {
        '+' => TokenKind::Plus,
        '-' => TokenKind::Minus,
        '*' => TokenKind::Star,
        '/' => TokenKind::Slash,
        '%' => TokenKind::Percent,
        '&' => TokenKind::Ampersand,
        '|' => TokenKind::Pipe,
        '^' => TokenKind::Caret,
        '~' => TokenKind::Tilde,
        '!' => TokenKind::Bang,
        '<' => TokenKind::Less,
        '>' => TokenKind::Greater,
        '=' => TokenKind::Equals,
        '.' => TokenKind::Dot,
        '(' => TokenKind::LeftParen,
        ')' => TokenKind::RightParen,
        '{' => TokenKind::LeftBrace,
        '}' => TokenKind::RightBrace,
        '[' => TokenKind::LeftBracket,
        ']' => TokenKind::RightBracket,
        ',' => TokenKind::Comma,
        ':' => TokenKind::Colon,
        ';' => TokenKind::Semicolon,
        '@' => TokenKind::At,
        '#' => TokenKind::Hash,
        _ => TokenKind::Error
    };
    
    let mut char_str = String::new();
    char_str.push(ch);
    let span = new_span(lexer.file_name.clone(), start_line, start_col, start, 1);
    let token = new_token(kind, span, char_str);
    LexerTokenResult { lexer: lexer, token: token }
}

/// Skip whitespace and comments.
/// Returns updated lexer
fn skip_whitespace_and_comments(mut lexer: Lexer) -> Lexer {
    while lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        
        // Whitespace
        if is_whitespace(ch) {
            lexer = advance(lexer);
            continue;
        }
        
        // Line comment: //
        if ch == '/' && lexer.position + 1 < string_length(lexer.source.clone()) {
            let next = char_at(lexer.source.clone(), lexer.position + 1);
            if next == '/' {
                // Skip until end of line
                while lexer.position < string_length(lexer.source.clone()) {
                    let c = char_at(lexer.source.clone(), lexer.position);
                    if c == '\n' {
                        break;
                    }
                    lexer = advance(lexer);
                }
                continue;
            }
            
            // Block comment: /* */
            if next == '*' {
                lexer = advance(lexer); // skip '/'
                lexer = advance(lexer); // skip '*'
                let mut depth = 1;
                
                while lexer.position < string_length(lexer.source.clone()) && depth > 0 {
                    let c = char_at(lexer.source.clone(), lexer.position);
                    
                    // Nested block comment start
                    if c == '/' && lexer.position + 1 < string_length(lexer.source.clone()) {
                        let n = char_at(lexer.source.clone(), lexer.position + 1);
                        if n == '*' {
                            depth = depth + 1;
                            lexer = advance(lexer);
                            lexer = advance(lexer);
                            continue;
                        }
                    }
                    
                    // Block comment end
                    if c == '*' && lexer.position + 1 < string_length(lexer.source.clone()) {
                        let n = char_at(lexer.source.clone(), lexer.position + 1);
                        if n == '/' {
                            depth = depth - 1;
                            lexer = advance(lexer);
                            lexer = advance(lexer);
                            continue;
                        }
                    }
                    
                    lexer = advance(lexer);
                }
                continue;
            }
        }
        
        // Not whitespace or comment
        break;
    }
    lexer
}

/// Advance position, tracking line and column.
/// Returns updated lexer
fn advance(mut lexer: Lexer) -> Lexer {
    if lexer.position < string_length(lexer.source.clone()) {
        let ch = char_at(lexer.source.clone(), lexer.position);
        if ch == '\n' {
            lexer.line = lexer.line + 1;
            lexer.column = 1;
        } else {
            lexer.column = lexer.column + 1;
        }
        lexer.position = lexer.position + 1;
    }
    lexer
}

/// Get the current character without advancing.
fn current_char_from_lexer(lexer: Lexer) -> char {
    if lexer.position < string_length(lexer.source.clone()) {
        char_at(lexer.source.clone(), lexer.position)
    } else {
        '\0'
    }
}

// Character classification helpers

fn is_whitespace(ch: char) -> bool {
    ch == ' ' || ch == '\t' || ch == '\n' || ch == '\r'
}

fn is_digit(ch: char) -> bool {
    ch >= '0' && ch <= '9'
}

fn is_hex_digit(ch: char) -> bool {
    (ch >= '0' && ch <= '9') || 
    (ch >= 'a' && ch <= 'f') || 
    (ch >= 'A' && ch <= 'F') ||
    ch == '_'
}

fn is_ident_start(ch: char) -> bool {
    (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') || ch == '_'
}

fn is_ident_continue(ch: char) -> bool {
    is_ident_start(ch) || is_digit(ch)
}

// String helper functions (Core-0 doesn't have string methods)

// Type conversion helpers for Core-0
// Note: Core-0 doesn't support the 'as' keyword for type casting.
// In the bootstrap context, the Stage 0 C# compiler handles these
// numeric type conversions implicitly during compilation. These wrapper
// functions exist solely to avoid using 'as' syntax which is not part
// of Core-0. When the Stage 1 Aster compiler is complete, it will need
// to implement proper conversion semantics for these functions.
fn usize_to_i32(val: usize) -> i32 {
    val
}

fn i32_to_usize(val: i32) -> usize {
    val
}

fn u8_to_char(byte: u8) -> char {
    byte
}

fn string_length(s: String) -> i32 {
    usize_to_i32(s.len())
}

fn char_at(s: String, index: i32) -> char {
    let bytes = s.as_bytes();
    if index >= 0 && i32_to_usize(index) < bytes.len() {
        u8_to_char(bytes[i32_to_usize(index)])
    } else {
        '\0'
    }
}

/// Lookup a keyword by text. Returns Identifier if not a keyword.
fn keyword_lookup(text: String) -> TokenKind {
    // In Core-0, we can't use HashMap, so we do manual comparison
    // This is a simplified version - in reality we'd check all keywords
    
    if text == "fn" { return TokenKind::Fn; }
    if text == "let" { return TokenKind::Let; }
    if text == "mut" { return TokenKind::Mut; }
    if text == "type" { return TokenKind::Type; }
    if text == "trait" { return TokenKind::Trait; }
    if text == "impl" { return TokenKind::Impl; }
    if text == "match" { return TokenKind::Match; }
    if text == "if" { return TokenKind::If; }
    if text == "else" { return TokenKind::Else; }
    if text == "for" { return TokenKind::For; }
    if text == "while" { return TokenKind::While; }
    if text == "return" { return TokenKind::Return; }
    if text == "break" { return TokenKind::Break; }
    if text == "continue" { return TokenKind::Continue; }
    if text == "async" { return TokenKind::Async; }
    if text == "await" { return TokenKind::Await; }
    if text == "actor" { return TokenKind::Actor; }
    if text == "module" { return TokenKind::Module; }
    if text == "pub" { return TokenKind::Pub; }
    if text == "extern" { return TokenKind::Extern; }
    if text == "unsafe" { return TokenKind::Unsafe; }
    if text == "using" { return TokenKind::Using; }
    if text == "managed" { return TokenKind::Managed; }
    if text == "throws" { return TokenKind::Throws; }
    if text == "struct" { return TokenKind::Struct; }
    if text == "enum" { return TokenKind::Enum; }
    if text == "true" { return TokenKind::True; }
    if text == "false" { return TokenKind::False; }
    
    TokenKind::Identifier
}
